# Awesome Multimodal Large Language Models Hallucination Mitigation
This is a list of some awesome works on mitigating hallucination in large multimodal models.



## :books:Survey

1. [Hallucination of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2404.18930) (Apr. 30, 2024)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.18930)[![github](https://img.shields.io/github/stars/showlab/Awesome-MLLM-Hallucination)](https://github.com/showlab/Awesome-MLLM-Hallucination/)



## :bar_chart:Benchmarks



## :clap:Hallucination Mitigation

1. [Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922) (Nov. 28, 2023, **CVPR 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.16922)[![github](https://img.shields.io/github/stars/DAMO-NLP-SG/VCD)](https://github.com/DAMO-NLP-SG/VCD)![tag](https://img.shields.io/badge/Highlight-FF4D00)![alias](https://img.shields.io/badge/VCD-black)
2. [OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation](https://arxiv.org/abs/2311.17911) (Nov. 29, 2023, **CVPR 2024**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17911)[![github](https://img.shields.io/github/stars/shikiw/OPERA)](https://github.com/shikiw/OPERA)![tag](https://img.shields.io/badge/Highlight-FF4D00)![alias](https://img.shields.io/badge/OPERA-black)
3. [PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training](https://arxiv.org/abs/2503.06486) (Mar. 9, 2025, **ICLR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.06486)![tag](https://img.shields.io/badge/Spotlight-FF4D00)![alias](https://img.shields.io/badge/PerturboLLaVA-black)
4.  [Mitigating Object Hallucination via Concentric Causal Attention](https://arxiv.org/abs/2410.15926) (Oct. 21, 2024, **NeurIPS 2024**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.15926)[![github](https://img.shields.io/github/stars/xing0047/cca-llava)](https://github.com/xing0047/cca-llava)![alias](https://img.shields.io/badge/CCA-black)
5. [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883) (Sep. 7, 2023, **ICLR 2024**) 
6. [VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification](https://arxiv.org/abs/2501.06553) (Jan. 11, 2025, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.06553)[![github](https://img.shields.io/github/stars/mengchuang123/VASparse-github)](https://github.com/mengchuang123/VASparse-github)![alias](https://img.shields.io/badge/VASparse-black)
7. [Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](https://arxiv.org/abs/2406.12718) (Jun. 18, 2024, **CVPR 2025**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.12718)[![github](https://img.shields.io/github/stars/Lackel/AGLA)](https://github.com/Lackel/AGLA)![alias](https://img.shields.io/badge/AGLA-black)
8. [Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models](http://arxiv.org/abs/2408.02032) (Aug. 4, 2024, **ICLR 2025**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.12718)[![github](https://img.shields.io/github/stars/huofushuo/SID)](https://github.com/huofushuo/SID)![alias](https://img.shields.io/badge/SID-black)
9. [DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination](https://arxiv.org/abs/2410.04514) (Oct. 6, **EMNLP 2024**)  [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.04514)![alias](https://img.shields.io/badge/DAMRO-black)



## :star:Acknowledgment

This project is inspired by [Awesome-MLLM-Hallucination](https://github.com/showlab/Awesome-MLLM-Hallucination). Thanks for their contribution to the research community.
